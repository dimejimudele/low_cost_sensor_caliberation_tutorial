{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11187c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095cb8c",
   "metadata": {},
   "source": [
    "Welcome to this lab session 3 on **Time series modeling for air pollution monitoring with a focus on the\n",
    "calibration of low-cost sensors.**\n",
    "\n",
    "This lab session is based on the data and methods provided in the study by [Ellen M. Considine et al](https://www.sciencedirect.com/science/article/pii/S0269749120365222).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97905f5",
   "metadata": {},
   "source": [
    "In the notebook, we will focus on using a regression models (linear model and random forest) to correct the readings of low-cost (CS) sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87e8646",
   "metadata": {},
   "source": [
    "We will start by presenting the use of a linear model for this purpose and we will explore different combinations of predictor variables in our model and compare the results. The idea here is to simulate a case of invastigating the best model and the features with which we get this model. \n",
    "\n",
    "In addition to a linear model, in our reference literature, a random forest model was also used. The follow-up task you will get in this session is to apply a random forest model and observe the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4a85d0",
   "metadata": {},
   "source": [
    "# First, lets import the libraries we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5844c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d410ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99416cca",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705cce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"./data/\"\n",
    "training_data_path = data_root + \"cleaned_training.csv\"\n",
    "test_data_path = data_root + \"cleaned_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6a8160",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(training_data_path)\n",
    "test_data = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b4ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94980482",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142b9df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More evaluation metrics can be added to the function\n",
    "def evaluate_model(y, y_hat):\n",
    "    evaluation = {\"RMSE\": round(mean_squared_error(y, y_hat, squared=False), 2)}\n",
    "    return evaluation\n",
    "\n",
    "\n",
    "# To disaggregate the error metrics to the different locations\n",
    "def get_disaggregated_metrics(data: \"dataframe\", y_hat: Optional[List] = None):\n",
    "\n",
    "    \"\"\"\n",
    "    Disaggregate evaluation metrics into locations\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for sensor in data[\"cs_sensor\"].unique():\n",
    "        data_sensor_idxs = data[data[\"cs_sensor\"] == sensor].index\n",
    "        true_data = data[[\"pm_airnow\"]][data.index.isin(data_sensor_idxs)]\n",
    "        pred_data = (\n",
    "            data[[\"pm_cs\"]][data.index.isin(data_sensor_idxs)]\n",
    "            if y_hat is None\n",
    "            else y_hat[data_sensor_idxs]\n",
    "        )\n",
    "\n",
    "        eval_metrics = evaluate_model(true_data, pred_data)\n",
    "        results[sensor] = eval_metrics\n",
    "\n",
    "    return {\"RMSE\": results}\n",
    "\n",
    "\n",
    "def run_training_eval(\n",
    "    features: dict,\n",
    "    train: \"dataframe\" = training_data,\n",
    "    test: \"dataframe\" = test_data,\n",
    "    estimator: \"sklearn model\" = LinearRegression(),\n",
    "):\n",
    "\n",
    "    \"\"\"\n",
    "    Run training and evaluation on the data\n",
    "\n",
    "    Purpose: fast experiment iteration\n",
    "    \"\"\"\n",
    "    y_train = train[[\"pm_airnow\"]]\n",
    "    X_train = training_data[features]\n",
    "\n",
    "    y_test = test[[\"pm_airnow\"]]\n",
    "    X_test = test[features]\n",
    "    \n",
    "    model = estimator\n",
    "    model.fit(X_train, y_train)\n",
    "    y_hat_test = model.predict(X_test)\n",
    "    y_hat_train = model.predict(X_train)  # predicted value\n",
    "\n",
    "    train_eval = evaluate_model(y_train, y_hat_train)\n",
    "    test_eval = evaluate_model(y_test, y_hat_test)\n",
    "\n",
    "    return {\n",
    "        \"train_eval\": train_eval,\n",
    "        \"test_eval\": test_eval,\n",
    "        \"model\": model,\n",
    "        \"y_hat_train\": y_hat_train,\n",
    "        \"y_hat_test\": y_hat_test,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6179cc",
   "metadata": {},
   "source": [
    "# How do we know we have a useful model? We need a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b4e93",
   "metadata": {},
   "source": [
    "Before we proceed into deriving correction models, we need to decide how we intend to evaluate our models. \n",
    "\n",
    "Our base study applied the coefficient of determination ($R^2$) and root-mean squared error (RMSE). \n",
    "\n",
    "RMSE shows us the mean distance between the predicted and actual values.\n",
    "\n",
    "For this notebook, we will be using RMSE measure which has been defined as part of our utility functions. Lower RMSE values indicate more accurate models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143bd730",
   "metadata": {},
   "source": [
    "We will obtain our baseline RMSE by computing the training and test RMSE between `pm_airnow` and `pm_cs`. Any model that will be useful whatsoever should do significantly better (i.e lower) than this baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2f78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_train_rmse = evaluate_model(training_data[\"pm_airnow\"], training_data[\"pm_cs\"])\n",
    "baseline_test_rmse = evaluate_model(test_data[\"pm_airnow\"], test_data[\"pm_cs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3d3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Baseline training evaluation is: {baseline_train_rmse}\")\n",
    "print(f\"Baseline test evaluation is: {baseline_test_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb9881",
   "metadata": {},
   "source": [
    "It is clear to us now that any model that learns something from our data should produce less than `5.51` and `7.08`RMSE over the training and test set, respectively. Please note that RMSE is on the scale of our data, meaning it is measured in $Âµg/m^3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de6d323",
   "metadata": {},
   "source": [
    "# Can we disaggregate these errors to the different sensors/locations?\n",
    "\n",
    "Starting from the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c56f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_disagg_train_metrics = get_disaggregated_metrics(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe14508",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Disaggregated RMSE score\")\n",
    "baseline_disagg_train_metrics[\"RMSE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b86303",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_disagg_test_metrics = get_disaggregated_metrics(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb880d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Disaggregated test RMSE\")\n",
    "baseline_disagg_test_metrics[\"RMSE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f0e70a",
   "metadata": {},
   "source": [
    "Again, any model that will be generally useful should do less than these errors in each of these locations (training and test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf897d74",
   "metadata": {},
   "source": [
    "# Obtaining our models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a780c77",
   "metadata": {},
   "source": [
    "**model_1**: CS PM2.5 <br>\n",
    "**model_2**: CS PM2.5, temperature, humidity <br>\n",
    "**model_3**: CS PM2.5, temprature, humidity, road length <br>\n",
    "**model_4**: CS PM2.5, temprature, humidity, road length, hour, month <br>\n",
    "**model_5**: CS PM2.5, temprature, humidity, road length, hour, mont, weekend <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34272a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = {\n",
    "    \"model_1\": [\"pm_cs\"],\n",
    "    \"model_2\": [\"pm_cs\", \"temp\", \"humidity\"],\n",
    "    \"model_3\": [\"pm_cs\", \"temp\", \"humidity\", \"a_road_500\"],\n",
    "    \"model_4\": [\n",
    "        \"pm_cs\",\n",
    "        \"temp\",\n",
    "        \"humidity\",\n",
    "        \"a_road_500\",\n",
    "        \"sin_time\",\n",
    "        \"cos_time\",\n",
    "        \"sin_month\",\n",
    "        \"cos_month\",\n",
    "    ],\n",
    "    \"model_5\": [\n",
    "        \"pm_cs\",\n",
    "        \"temp\",\n",
    "        \"humidity\",\n",
    "        \"a_road_500\",\n",
    "        \"sin_time\",\n",
    "        \"cos_time\",\n",
    "        \"sin_month\",\n",
    "        \"cos_month\",\n",
    "        \"weekend\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af8e25",
   "metadata": {},
   "source": [
    "###### Let us run our first example using only one feature `pm_cs` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee4524",
   "metadata": {},
   "source": [
    "We need to do the following:\n",
    "- Choose the features that we will be using to fit our model\n",
    "- Extract `x_train, x_test, y_train, y_test`, as features and reponse variables for  training and test.\n",
    "- Fit the model\n",
    "- Predict\n",
    "- Evaluate: RMSE and scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec41eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = model_features[\"model_1\"]\n",
    "\n",
    "y_train = training_data[[\"pm_airnow\"]]\n",
    "X_train = training_data[features]\n",
    "\n",
    "y_test = test_data[[\"pm_airnow\"]]\n",
    "X_test = test_data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723894ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ea5cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the predicted value as y_hat\n",
    "y_hat_test = model.predict(X_test)\n",
    "y_hat_train = model.predict(X_train)  # predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3935422",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_hat_test, label=\"Corrected\", alpha=0.3)\n",
    "plt.scatter(y_test, test_data[[\"pm_cs\"]], label=\"Not corrected\", alpha=0.3)\n",
    "plt.xlabel(\"Airnow\")\n",
    "plt.ylabel(\"CS\")\n",
    "plt.title(\"Test data evaluation plot\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a8bac7",
   "metadata": {},
   "source": [
    "**We should only have only one model coefficient here, so we can't actually compare features. This step will be useful once we start scaling into more features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db872b6",
   "metadata": {},
   "source": [
    "**Now, let us evaluate and compare to baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d8d4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train_rmse = evaluate_model(y_train, y_hat_train)\n",
    "model_test_rmse = evaluate_model(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65532800",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Resulting training evaluation is: {model_train_rmse} compared to baseline which is: {baseline_train_rmse}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Resulting test evaluation is: {model_test_rmse} compared to baseline which is: {baseline_test_rmse}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec613e08",
   "metadata": {},
   "source": [
    "Our model reduces our error both in training and test compared to the baseline we obtained initially. This shows that we have already made meaningful progress using a linear model. It is a good start.\n",
    "\n",
    "Now, let us observe the errors for each location (disaggregated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185018b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_disagg_train_metrics = get_disaggregated_metrics(training_data, y_hat_train)\n",
    "model_disagg_test_metrics = get_disaggregated_metrics(test_data, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b017f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model training RMSE\")\n",
    "model_disagg_train_metrics[\"RMSE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73219e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model test RMSE\")\n",
    "model_disagg_test_metrics[\"RMSE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62bd627",
   "metadata": {},
   "source": [
    "Our model reduces our RMSE error with respect to the baseline; we are correcting the low-cost sensor readings with about 50% reduction in error in `i25_denver` and 30% in `CAMP`. Data from these locations have not been seen by our model. There is some progress, right? See table below.\n",
    "\n",
    "  | Location| Baseline RMSE| Linear model 1 RMSE|\n",
    "  |---|---|---|\n",
    "   |**Train**|---|---|\n",
    "  |NJH | 4.36| 2.28|\n",
    "  |i25_glo_1|6.67|3.97|\n",
    "  |i25_glo_2|4.55|3.57|\n",
    "  |i25_glo_3|5.41|3.65|\n",
    "  |la_casa|6.06|2.86|\n",
    "   |**Train**|---|---|\n",
    "  |CAMP|2.35|1.64|\n",
    "  |i25_denver|8.04|4.47|\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db0f589",
   "metadata": {},
   "source": [
    "# EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a0cc0a",
   "metadata": {},
   "source": [
    "We need to repeat this process above for other feature sets defined in `model_features`.\n",
    "\n",
    "For faster iterations, you can use `run_training_eval` function to iterate over features. Take not that the `run_training_eval` function also takes an `Ã¨stimator` argument which defaults to `LinearRegressor()`. You can change the value passed to this argument to `RandomForestRegressor` (which has alrready been imported into this notebook). This way, you can try they random forest model.  \n",
    "\n",
    "Start with exploring all the possible features defined in `model_features` for `LinearRegressor()` before proceeding to `RandomForestRegressor`.\n",
    "\n",
    "Find out what your best model is and deepen your exploration of this model using things like scatter plots for camparing corrected and not corrected `pm_cs`values. You model output is the corrected `pm_cs`value.\n",
    "\n",
    "You can also find the feature importances by using `.feature_importances_`on your trained model. If you apply `run_training_eval`to fit your model, you will find the trained model in by using the `model`on the output dictionary.\n",
    "\n",
    "You can then proceed to check for disaggregated errors for your best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea96cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear model\")\n",
    "for features in model_features:\n",
    "    results = run_training_eval(model_features[features])\n",
    "    print(f\"Results for {features}\")\n",
    "    print(\"========================\")\n",
    "    print({\"Train\": results[\"train_eval\"]})\n",
    "    print({\"Test\": results[\"test_eval\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ffc1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba837500",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest\")\n",
    "\n",
    "for features in model_features:\n",
    "    results = run_training_eval(model_features[features], model=RandomForestRegressor())\n",
    "    print(f\"Results for {features}\")\n",
    "    print(\"========================\")\n",
    "    print({\"Train\": results[\"train_eval\"]})\n",
    "    print({\"Test\": results[\"test_eval\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920b8832",
   "metadata": {},
   "source": [
    "The best model obtained is from based on random forest using `[\"pm_cs\", \"temp\", \"humidity\", \"a_road_500\", \"sin_time\" \"cos_time\", \"sin_month\", \"cos_month\"]`, i.e `model_4`\n",
    "\n",
    "The result for our best model is:\n",
    "\n",
    "`train RMSE= 0.85` <br>\n",
    "`test RMSE= 3.46`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca70cdab",
   "metadata": {},
   "source": [
    "###### Now, let us deepen our best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ee5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_best_model = run_training_eval(\n",
    "    model_features[\"model_4\"], model=RandomForestRegressor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99fa36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    test_data[\"pm_airnow\"],\n",
    "    results_best_model[\"y_hat_test\"],\n",
    "    alpha=0.4,\n",
    "    label=\"Corrected\",\n",
    ")\n",
    "plt.scatter(\n",
    "    test_data[\"pm_airnow\"], test_data[\"pm_cs\"], alpha=0.4, label=\"Not corrected\"\n",
    ")\n",
    "plt.ylabel(\"AirNow\")\n",
    "plt.xlabel(\"CS\")\n",
    "plt.title(\"Test prediction from best model\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be3ed2c",
   "metadata": {},
   "source": [
    "###### Now, we should check the feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fbcdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = results_best_model[\"model\"].feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d760b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 4))\n",
    "plt.bar(model_features[\"model_4\"], feature_importances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3831fa",
   "metadata": {},
   "source": [
    "The most important feature for our random forest correction model is the `PM 2.5`readings from the low-cost sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcd5950",
   "metadata": {},
   "source": [
    "**Disaggregating the errors into the different locations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_disagg_test_metric = get_disaggregated_metrics(\n",
    "    test_data, results_best_model[\"y_hat_test\"]\n",
    ")\n",
    "best_disagg_train_metric = get_disaggregated_metrics(\n",
    "    training_data, results_best_model[\"y_hat_train\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c981f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best model\")\n",
    "print(best_disagg_test_metric[\"RMSE\"])\n",
    "print(\"baseline\")\n",
    "print(baseline_disagg_test_metrics[\"RMSE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef187d7",
   "metadata": {},
   "source": [
    "Compare to our baseline for test data as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c85a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"best model\")\n",
    "print(best_disagg_train_metric[\"RMSE\"])\n",
    "print(\"baseline\")\n",
    "print(baseline_disagg_train_metrics[\"RMSE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a916b2",
   "metadata": {},
   "source": [
    "We have lower errors in NJH and CAMP which are locations with lowers IQRs. They are both farther from the expressway than the other locations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490af6af",
   "metadata": {},
   "source": [
    "Best result compared to baseline:\n",
    "\n",
    "| Location| Baseline RMSE| Random forest RMSE|\n",
    "  |---|---|---|\n",
    "  |**Train**|---|---|\n",
    "  |NJH | 4.36| 0.63|\n",
    "  |i25_glo_1|6.67|1.01|\n",
    "  |i25_glo_2|4.55|0.93|\n",
    "  |i25_glo_3|5.41|0.86|\n",
    "  |la_casa|6.06|0.68|\n",
    "  |**Test**|---|---|\n",
    "  |CAMP|2.35|1.64|\n",
    "  |i25_denver|8.04|4.47|\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d208729",
   "metadata": {},
   "source": [
    "Remember that the training results are obtained from data and locations that have already been seen by the model. Hence we have results that are over-optimistic for those locations. The model we have so far, however, shows usefulness in that it reduces error on test data (which are locations that have not been seen by our our model).\n",
    "\n",
    "How do we objectively evaluate the performance of our model in locations that are in the training data?\n",
    "For this, we will apply LOOO cross validation in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba72c33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
